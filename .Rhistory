# Para aplicar un modelo regularizado podemos usar la función `glmnet::glmnet()`. El parámetro `alpha` le dice a glmnet que realice un modelo ridge (`alpha` = 0), lasso (`alpha` = 1), o elastic net (0 < `alpha` < 1).
# Por defecto, `glmnet` hará dos cosas :
### 1. Estandarización automática
# Dado que los métodos regularizados aplican una penalización a los coeficientes, necesitamos asegurar que nuestros coeficientes estén en una escala común. Si no es así, entonces los predictores con valores naturalmente más grandes serán penalizados más que los predictores con valores naturalmente más pequeños. Por defecto, `glmnet` estandariza automáticamente tus variables. Si estandarizas tus predictores antes de usar glmnet, puedes desactivar este argumento con `standardize = FALSE`.
### 2. Secuencia automática de λ (lambda)
# `glmnet` ajustará modelos a través de un amplio rango de valores de λ (lambda)
## Puntos Clave sobre glmnet
### 1. **Estandarización es Crucial**
# Sin estandarizar: variables con unidades de medida similar dominaría la penalización
# Con estandarizar: todas las variables están en escala similar
### 2. **Secuencia Automática de λ**
# glmnet automáticamente crea ~100 valores de λ desde muy grande (todos los coeficientes = 0) hasta muy pequeño (modelo sin regularizar).
### 3. **Parámetro Alpha**
# - `alpha = 0`: **Ridge** - reduce coeficientes pero no los elimina
# - `alpha = 1`: **Lasso** - puede eliminar variables (coeficientes = 0)
# - `alpha = 0.5`: **Elastic Net** - combina ambos enfoques
### 4. **Dos λ importantes en cv.glmnet**
# - `lambda.min`: El λ que minimiza el error de validación cruzada
# - `lambda.1se`: Un λ más grande (modelo más simple) dentro de 1 error estándar del mínimo
# --------------------------------------------------------
# Ejemplo en modelo sencillo
# --------------------------------------------------------
modellev2
# Glmnet solo acepta matrices
X<- model.matrix(formula(modellev2),data_train)[, -1]  # Eliminar intercepto
y<- data_train$price
lasso_fit <- glmnet(
x = X,
y = y,
alpha = 1 #ridge
)
# graficamos
plot(lasso_fit,xvar="lambda")
# Gráfico mas bonito
# Con ggplot
coef_matrix <- as.matrix(coef(lasso_fit))[-1, ]  # [-1, ] elimina intercepto
coef_df <- coef_matrix %>%
as.data.frame() %>%
rownames_to_column("variable") %>%
pivot_longer(-variable, names_to = "lambda_index", values_to = "coefficient") %>%
mutate(
lambda_index = as.numeric(str_remove(lambda_index, "s")),
lambda = lasso_fit$lambda[lambda_index + 1],
log_lambda = log(lambda)
)
final_coefs <- coef_df %>%
filter(lambda_index == max(lambda_index)) %>%
filter(abs(coefficient) > 1e-6) %>%  # Solo variables con coeficiente != 0
arrange(desc(abs(coefficient)))
lasso_path_plot <- coef_df %>%
ggplot(aes(x = log_lambda, y = coefficient, color = variable)) +
geom_line(size = 1, alpha = 0.8) +
# Agregar etiquetas al final de las líneas
geom_text(
data = final_coefs,
aes(label = variable),
hjust = -0.4,
vjust= 1,
size = 3,
show.legend = FALSE
) +
# Línea vertical en y = 0 para referencia
geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.5) +
# Personalizar ejes y tema
labs(
title = "Trayectoria de Coeficientes - Lasso",
subtitle = "Cómo cambian los coeficientes con diferentes valores de λ",
x = "log(λ)",
y = "Coeficientes",
caption = "Las líneas muestran cómo cada coeficiente se reduce hacia cero"
) +
theme_minimal() +
theme(
legend.position = "none",  # Quitamos leyenda ya que tenemos etiquetas
plot.title = element_text(size = 14, face = "bold"),
plot.subtitle = element_text(size = 12),
axis.title = element_text(size = 11),
panel.grid.minor = element_blank()
) +
# Expandir límites del plot para que se vean las etiquetas
coord_cartesian(xlim = c(min(coef_df$log_lambda),
max(coef_df$log_lambda) + 0.5))
lasso_path_plot
# Grafico interactivo
p_load(plotly)
#
interactive_plot <- ggplotly(lasso_path_plot, tooltip = c("x", "y", "colour"))
interactive_plot
# --------------------------------------------------------
#  Selección de lambda
# --------------------------------------------------------
set.seed(123)  # Para reproducibilidad
# Validación cruzada con Lasso
cv_lasso <- cv.glmnet(
x = X,
y = y,
alpha = 1,           # Lasso
nfolds = 10,         # 10-fold CV (puedes cambiar a 5 si prefieres)
type.measure = "mse"
)
# Lambda minimo
lambda_min <- cv_lasso$lambda.min    # Mínimo error CV
lambda_min
#lambda_1se <- cv_lasso$lambda.1se    # Regla 1 error estándar
# Graficamos
plot(cv_lasso)
# Estimamos el modelo para el lambda minimo
lasso_min <- glmnet(X, y, alpha = 1, lambda = lambda_min)
# Extraer coeficientes
coef_min <- coef(lasso_min, s = lambda_min)
coef_min
# Comparar número de variables seleccionadas
n_vars_min <- sum(coef_min[-1] != 0)  # Excluir intercepto
n_vars_min
# -------------------------------------------------------
# Performance en los datos de prueba
# -------------------------------------------------------
# Crear matriz X para datos de prueba (MISMA ESTRUCTURA que entrenamiento)
X_test <- model.matrix(formula(modellev2), data_test)[, -1]
y_test <- data_test$price  # Variable objetivo real en test
# Predicciones con lambda.min
pred_min <- predict(cv_lasso, newx = X_test, s = "lambda.min")
pred_min <- as.numeric(pred_min)  # Convertir a vector
# Predicciones con lambda.1se
pred_1se <- predict(cv_lasso, newx = X_test, s = "lambda.1se")
pred_1se <- as.numeric(pred_1se)
# Función para calcular múltiples métricas
calculate_metrics <- function(y_true, y_pred, model_name) {
residuals <- y_true - y_pred
metrics <- tibble(
modelo = model_name,
MSE = mean(residuals^2),
)
return(metrics)
}
# Calcular métricas para ambos modelos
metrics_min <- calculate_metrics(y_test, pred_min, "lambda.min")
metrics_1se <- calculate_metrics(y_test, pred_1se, "lambda.1se")
metrics_min
metrics_1se
lapply(model_list,predictor)
p_load(tidyverse, Matrix, glmnet)
# 0) Asegura factores con niveles consistentes entre train/test
to_fac <- c("f_property_type", "f_bed_type")
for (v in to_fac) {
lvls <- union(unique(data_train[[v]]), unique(data_test[[v]]))
data_train[[v]] <- factor(data_train[[v]], levels = lvls)
data_test[[v]]  <- factor(data_test[[v]],  levels = lvls)
}
# 1) Bloques de M7 (ya definidos arriba)
# basic_lev, basic_add, reviews, poly_lev, X1, X2, amenities
# 2) Construye M8 con "*" pero quitando efectos principales extras
amen_sum   <- paste(amenities, collapse = " + ")
base_block <- paste(c(basic_lev, basic_add, reviews, poly_lev, X1, X2, amenities),
collapse = " + ")
# Interacciones deseadas usando "*":
# (amenities)*(f_property_type + f_bed_type) - amenities - f_property_type - f_bed_type
inter_block <- paste0(
"(", amen_sum, ")*(", "f_property_type + f_bed_type", ")",
" - (", amen_sum, ") - (f_property_type + f_bed_type)"
)
modellev8 <- paste0(" ~ ", base_block, " + ", inter_block)
modellev8
# 3) Matrices de diseño dispersas (sin terms())
fmla8_rhs <- as.formula(modellev8)
X8      <- sparse.model.matrix(fmla8_rhs, data_train)[, -1, drop = FALSE]  # sin intercepto
X8_test <- sparse.model.matrix(fmla8_rhs, data_test)[,  -1, drop = FALSE]
# 4) Alinear columnas (espacio de features idéntico)
train_cols <- colnames(X8)
test_cols  <- colnames(X8_test)
# Quita columnas "extra" en test (niveles no vistos en train)
extra_in_test <- setdiff(test_cols, train_cols)
if (length(extra_in_test) > 0) {
X8_test <- X8_test[, setdiff(test_cols, extra_in_test), drop = FALSE]
}
# Agrega columnas faltantes en test con ceros
missing_in_test <- setdiff(train_cols, colnames(X8_test))
if (length(missing_in_test) > 0) {
add0 <- Matrix(0, nrow(X8_test), length(missing_in_test), sparse = TRUE,
dimnames = list(NULL, missing_in_test))
X8_test <- cBind(X8_test, add0)
}
# Reordenar columnas de test como train
X8_test <- X8_test[, train_cols]
# 5) Respuesta
y8      <- data_train$price
y8_test <- data_test$price
# 6) LASSO con CV (idéntico a tu flujo)
set.seed(123)
cv_lasso_m8 <- cv.glmnet(
x = X8,
y = y8,
alpha = 1,           # LASSO
nfolds = 10,
type.measure = "mse",
standardize = TRUE
)
lambda_min_m8 <- cv_lasso_m8$lambda.min
lambda_1se_m8 <- cv_lasso_m8$lambda.1se
# 7) Nº de coeficientes distintos de cero (excluye intercepto)
coef_min_m8 <- coef(cv_lasso_m8, s = "lambda.min")
coef_1se_m8 <- coef(cv_lasso_m8, s = "lambda.1se")
n_vars_min_m8 <- sum(as.numeric(coef_min_m8[-1]) != 0)
n_vars_1se_m8 <- sum(as.numeric(coef_1se_m8[-1]) != 0)
# 8) MSE fuera de muestra
pred_min_m8 <- as.numeric(predict(cv_lasso_m8, newx = X8_test, s = "lambda.min"))
pred_1se_m8 <- as.numeric(predict(cv_lasso_m8, newx = X8_test, s = "lambda.1se"))
mse_min_m8 <- mean((y8_test - pred_min_m8)^2)
mse_1se_m8 <- mean((y8_test - pred_1se_m8)^2)
# 9) Resumen
results_lasso_m8 <- tibble(
modelo         = c("M8-LASSO (lambda.min)", "M8-LASSO (lambda.1se)"),
lambda         = c(lambda_min_m8,            lambda_1se_m8),
n_coef_no_cero = c(n_vars_min_m8,            n_vars_1se_m8),
MSE_test       = c(mse_min_m8,               mse_1se_m8)
)
results_lasso_m8
require("pacman")
p_load("tidyverse","vtable","stargazer")
dta<- read_csv("https://raw.githubusercontent.com/ignaciomsarmiento/datasets/refs/heads/main/ads_data.csv")
st(dta, group = 'd',group.test = TRUE)
# Modelo lineal simple
mod1 <- lm(lny ~ d, data = dta)
summary(mod1)
# Modelo lineal con controles
mod2 <- lm(lny ~ d + age + male,data=dta)
summary(mod2)
stargazer(mod1, mod2,
type = "html",
title = "Efectos de la Publicidad Digital en el Gasto",
column.labels = c("Modelo Base", "Modelo Completo"),
covariate.labels = c("Tratamiento (Anuncios)", "Edad", "Hombre",
"Constante"),
dep.var.labels = "Log(Gasto Mensual)",
notes = "Errores estándar en paréntesis. *** p<0.01, ** p<0.05, * p<0.1")
setwd("~/Desktop/andes/12do semestre/Ciencia de datos y econometría aplicada/Taller4_PIB_Regional_italiano")
# =========================================================
#
#
#
#
#
#
#
#
#
#
# =========================================================
# =========================================================
# Part 0: Cargar datos, buenas prácticas y librerías
# =========================================================
# setwd("~/Desktop/Taller 1 - BigData/Taller4_PIB_Regional_italiano")
# setwd("C:/Users/Asuar/OneDrive/Escritorio/Libros Clases/Economía/Ciencia Datos y Econometria/Taller4_PIB_Regional_italiano")
rm(list = ls())
require(pacman)
p_load(sf, tidyverse, dplyr, knitr, stargazer, ggplot2, stringr,
spdep, spatialreg)
# Cargar datos espaciales
italia_espaciales <- st_read("stores/Reg2014_ED50g/Reg2014_ED50_g.shp")
# Cargar datos csv
GDP_ita <- read_csv("stores/Data.csv")
# =========================================================
# Part 1: Análisis descriptivo y territorial
# =========================================================
# Estadísticas descriptvas de regiones de Italia
vars <- c("GDP", "K", "L")
#  Tabla de regiones ordenadas por GDP
tabla_regiones <- GDP_ita[order(GDP_ita$GDP, decreasing = TRUE), c("Territory", vars)]
print(tabla_regiones)
# Estadísticas descriptivas generales de las variables
tabla_summary <- do.call(rbind, lapply(GDP_ita[vars], summary))
# Mostrar con nombres más limpios
tabla_summary <- as.data.frame(tabla_summary)
tabla_summary
# Mapas temáticos usando polígonos regionales
# Arreglar problema nombres previo a join
italia_norm <- italia_espaciales %>%
mutate(
REGIONE_raw = REGIONE,
REGIONE = REGIONE %>%
str_replace_all("[-–—]", " ")
)
# Unir shapefile con datos económicos
italia_map <- italia_norm %>%
left_join(GDP_ita, by = c("REGIONE" = "Territory"))
# --------------------------------------------------------
# Función auxiliar para mapas
plot_map <- function(data, variable, titulo) {
ggplot(data) +
geom_sf(aes(fill = !!sym(variable)), color = "white", size = 0.2) +
scale_fill_viridis_c(option = "plasma", direction = -1) +
labs(title = titulo, fill = variable) +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
legend.position = "right"
)
}
# Mapas por variables: GDP, K, L
map_GDP <- plot_map(italia_map, "GDP", "Producto Interno Bruto (GDP) por región")
map_K   <- plot_map(italia_map, "K", "Capital (K) por región")
map_L   <- plot_map(italia_map, "L", "Trabajo (L) por región")
# Mostrar en consola
map_GDP
map_K
map_L
# --------------------------------------------------------
# Exploración de correlaciones simples entre variables
pairs(GDP_ita[vars],
main = "Matriz de dispersión entre variables económicas",
pch = 19, col = "blue")
# =========================================================
# Part 2: Modelo base (MCO)
# =========================================================
# Estimación Cobb-Douglas en forma log-lineal
model_CB <- lm( log(GDP)~ log(K) + log(L), data=GDP_ita)
summary(model_CB)
# Suma de elasticidades
elasticidad_suma <- sum(coef(model_CB)[-1])
elasticidad_suma
# =========================================================
# Part 3: Analisis de dependencia espacial
# =========================================================
# Verificar el sistema de coordenadas
st_crs(italia_map)
## De acuerdo a este codigo, el archivo ya se encuentra en metros
# --- 3.1 Construcción de matriz de pesos espaciales (W) ---
# Obtener centroides de las regiones
coords <- st_centroid(st_geometry(italia_map))
# Probando diferentes criteros para encontrar el punto optimo donde cada punto tenga al
# menos un vecino 50, 100, 200, 300, 350, 370, 379
# CRITERIO 1: Matriz por DISTANCIA (umbral escogido = 379 km)
# Nota: Ajusta el umbral según la escala de tus datos
dist_380km <- spdep::dnearneigh(coords, 0, 379000) # 379,000 metros
# Verificar regiones sin vecinos
n_sin_vecinos_dist <- sum(card(dist_380km) == 0)
print(paste("=== MATRIZ DE PESOS POR DISTANCIA ==="))
print(paste("Umbral: 379 km"))
print(paste("Regiones sin vecinos:", n_sin_vecinos_dist))
# Si hay regiones sin vecinos, mostrar cuáles son
if(n_sin_vecinos_dist > 0) {
regiones_sin_vecinos <- italia_map$REGIONE[which(card(dist_380km) == 0)]
print(paste("Regiones sin vecinos:", paste(regiones_sin_vecinos, collapse = ", ")))
}
# Crear matriz de pesos W (row-standardized)
W_dist <- spdep::nb2listw(dist_380km, style = "W", zero.policy = TRUE)
# --- 3.2 Dispersión y Número promedio de vecinos
summary(dist_380km)
# Numero promedio de vecinos es 8.2
# Sparsity seria 41%
# --- 3.3 Test de Moran´s
# Primero con residuos de MCO
residuos_mco <- residuals(model_CB)
moran_dist <- spdep::moran.test(residuos_mco, W_dist,
alternative = "two.sided",
zero.policy = TRUE)
print(moran_dist)
# P-Value = 5.17e-09 - Significativo, Si hay autocorrelación espacial, El modelo MCO **NO capturó** toda la estructura espacial
# Hay efectos espaciales (spillovers, externalidades) que el modelo ignora.
# Segundo con la variable GDP
moran_gdp <- spdep::moran.test(italia_map$GDP, W_dist,
alternative = "two.sided",
zero.policy = TRUE)
print(moran_gdp)
# P-Value = 0.4592 - No es significativo, No hay autocorrelación espacial
# =========================================================
# Part 4: Modelos Espaciales
# =========================================================
# --- 4.1 Modelo Spatial Lag (SAR) ---
model_SAR <- lagsarlm(log(GDP) ~ log(K) + log(L),
data = italia_map,
listw = W_dist)
summary(model_SAR)
# --- 4.2 Modelo SARAR (SAC) ---
model_SAC <- sacsarlm(log(GDP) ~ log(K) + log(L),
data = italia_map,
listw = W_dist)
summary(model_SAC)
# --- 4.3 Comparación de modelos ---
# AIC y BIC para comparar
AIC(model_CB, model_SAR, model_SAC)
BIC(model_CB, model_SAR, model_SAC)
lm.LMtests(model_CB, W_dist, test = "all")
# Interpretación:
# - La dependencia espacial está PRINCIPALMENTE en los ERRORES (error espacial)
# - También hay algo de dependencia en lag, pero es secundaria
# - SARMA significativo -> usar modelo SAC
# Modelo recomendado: SAC (SARAR)
# Razón: Captura tanto el error espacial (dominante) como algo de lag espacial
# =========================================================
# Part 5: Análisis de Impactos y Diagnóstico Territorial
# =========================================================
# --- 5.1 Impactos Directos, Indirectos y Totales ---
# El modelo SAC proporciona impactos a través de la matriz de multiplicadores espaciales
# Para SAC: Impactos = (I - ρW)^-1 * [I, β_K*I, β_L*I]
# Cálculo de impactos espaciales con bootstrap
imp_SAC <- impacts(model_SAC, listw = W_dist, R = 2000)
# Resumen
summary(imp_SAC)
# ---- Impactos medios ----
d_mean   <- imp_SAC$res$direct[,1]
ind_mean <- imp_SAC$res$indirect[,1]
tot_mean <- imp_SAC$res$total[,1]
# ---- Desviaciones estándar ----
d_sd   <- apply(imp_SAC$sres$direct,   2, sd)
ind_sd <- apply(imp_SAC$sres$indirect, 2, sd)
tot_sd <- apply(imp_SAC$sres$total,    2, sd)
# ---- Variables ----
vars <- c("log(K)", "log(L)")
# ---- Construir tabla con "media (sd)" ----
impactos_tabla <- data.frame(
Variable   = vars,
Directo    = sprintf("%.4f (%.4f)", d_mean,   d_sd),
Indirecto  = sprintf("%.4f (%.4f)", ind_mean, ind_sd),
Total      = sprintf("%.4f (%.4f)", tot_mean, tot_sd)
)
# ---- Mostrar tabla ----
knitr::kable(
impactos_tabla,
caption = "Impactos Directos, Indirectos y Totales con Desviaciones Estándar – Modelo SAC"
)
# --- 5.2 Identificación de Regiones con Residuos Atípicos ---
# Residuos del modelo
italia_map$Residuos_SAC <- residuals(model_SAC)
# Residuos estandarizados
italia_map$Residuos_std <- scale(italia_map$Residuos_SAC)[,1]
# Regiones con peor desempeño (muy negativo)
head(
italia_map[order(italia_map$Residuos_std),
c("REGIONE", "Residuos_SAC","Residuos_std")],
5
)
# Regiones con mejor desempeño (muy positivo)
head(
italia_map[order(-italia_map$Residuos_std),
c("REGIONE", "Residuos_SAC","Residuos_std")],
5
)
# --- 5.3 Visualización: Mapa de Residuos ---
ciudades_italia <- data.frame(
ciudad = c(
# Grandes ciudades
"Roma","Milano","Napoli","Torino","Palermo","Genova","Bologna","Firenze","Bari","Catania",
# Capitales regionales
"L'Aquila","Catanzaro","Potenza","Ancona","Trieste","Campobasso","Trento","Bolzano",
"Aosta","Perugia","Venezia", "Cagliari"
),
lon = c(
12.4964, 9.1900, 14.2681, 7.6869, 13.3613, 8.9463, 11.3426, 11.2558, 16.8719, 15.0873,
13.3995, 16.6050, 15.8050, 13.5150, 13.7768, 14.6620, 11.1211, 11.3548,
7.3151, 12.3889, 12.3155, 9.1100
),
lat = c(
41.9028, 45.4642, 40.8518, 45.0703, 38.1157, 44.4056, 44.4949, 43.7696, 41.1171, 37.5022,
42.3499, 38.8890, 40.6395, 43.6168, 45.6495, 41.5620, 46.0707, 46.4983,
45.7370, 43.1122, 45.4408, 39.2170
)
)
# Converir a sf para poder usar
ciudades_sf <- st_as_sf(
ciudades_italia,
coords = c("lon","lat"),
crs = 4326
)
# Mapa de residuos
mapa_residuos <- ggplot(italia_map) +
geom_sf(aes(fill = Residuos_SAC), color = "white", size = 0.2) +
geom_sf(data = ciudades_sf, color = "black", size = 2) +
geom_sf_label(
data = ciudades_sf,
aes(label = ciudad),
size = 3,
label.size = 0.2,
alpha = 0.7
) +
scale_fill_gradient2(
low = "blue",
mid = "white",
high = "red",
midpoint = 0
) +
labs(
title = "Residuos del Modelo SAC por Región",
subtitle = "Valores positivos: el modelo subestima el PIB. Valores negativos: lo sobreestima.",
fill = "Residuos"
) +
theme_minimal()
mapa_residuos
# Mapa de residuos estandarizados
mapa_residuos_std <- ggplot(italia_map) +
geom_sf(aes(fill = Residuos_std), color = "white", size = 0.2) +
geom_sf(data = ciudades_sf, color = "black", size = 2) +
geom_sf_label(
data = ciudades_sf,
aes(label = ciudad),
size = 3,
label.size = 0.2,
alpha = 0.7
) +
scale_fill_gradient2(
low = "blue",
mid = "white",
high = "red",
midpoint = 0
) +
labs(
title = "Residuos Estandarizados (z-score)",
subtitle = "Valores > 2 indican potenciales outliers espaciales",
fill = "Residuos Std"
) +
theme_minimal()
mapa_residuos_std
